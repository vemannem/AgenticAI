{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Required libraries\n",
        "\n",
        "#### langchina, langchain_openai, langchain_community : core langchain libraries for buidling RAG based application that includes document loader, text splitter, create embeddings, create and store vector db, initiate and connect LLM, provide document chain and rag chain .\n",
        "\n",
        "#### openAI : official python library for openAI . used by langchain for interacting with openAI LLMs\n",
        "\n",
        "#### pinecone-client : official pinecone library for interacting with vector db\n",
        "\n"
      ],
      "metadata": {
        "id": "3yjo1zcuVtyg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYnHuA0UUV2T"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet langchain langchain-openai langchain-community openai  pinecone-client pypdf python-dotenv \"unstructured[md,txt,pdf]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio_client==0.2.10\n",
        "!pip install gradio==3.38.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S1W_1AU_M-8q",
        "outputId": "0365a83a-6e91-4e45-f06d-73def36d9984"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio_client==0.2.10\n",
            "  Downloading gradio_client-0.2.10-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio_client==0.2.10) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from gradio_client==0.2.10) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from gradio_client==0.2.10) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio_client==0.2.10) (24.2)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio_client==0.2.10) (2.32.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio_client==0.2.10) (4.15.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio_client==0.2.10)\n",
            "  Downloading websockets-11.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13.0->gradio_client==0.2.10) (3.20.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13.0->gradio_client==0.2.10) (6.0.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13.0->gradio_client==0.2.10) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13.0->gradio_client==0.2.10) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.0->gradio_client==0.2.10) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests~=2.0->gradio_client==0.2.10) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.0->gradio_client==0.2.10) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests~=2.0->gradio_client==0.2.10) (2025.10.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->gradio_client==0.2.10) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->gradio_client==0.2.10) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->gradio_client==0.2.10) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx->gradio_client==0.2.10) (1.3.1)\n",
            "Downloading gradio_client-0.2.10-py3-none-any.whl (288 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-11.0.3-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets, gradio_client\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: gradio_client\n",
            "    Found existing installation: gradio_client 1.13.3\n",
            "    Uninstalling gradio_client-1.13.3:\n",
            "      Successfully uninstalled gradio_client-1.13.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.17.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 11.0.3 which is incompatible.\n",
            "yfinance 0.2.66 requires websockets>=13.0, but you have websockets 11.0.3 which is incompatible.\n",
            "gradio 5.49.1 requires gradio-client==1.13.3, but you have gradio-client 0.2.10 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires websockets>=14.0, but you have websockets 11.0.3 which is incompatible.\n",
            "google-genai 1.46.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 11.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gradio_client-0.2.10 websockets-11.0.3\n",
            "Collecting gradio==3.38.0\n",
            "  Downloading gradio-3.38.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==3.38.0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: aiohttp~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (3.13.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (5.5.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (0.120.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (0.6.4)\n",
            "Requirement already satisfied: gradio-client>=0.2.10 in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (0.2.10)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (3.1.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.38.0) (4.0.0)\n",
            "Collecting markupsafe~=2.0 (from gradio==3.38.0)\n",
            "  Downloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (3.10.0)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio==3.38.0)\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting numpy~=1.0 (from gradio==3.38.0)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (2.2.2)\n",
            "Collecting pillow<11.0,>=8.0 (from gradio==3.38.0)\n",
            "  Downloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (6.0.3)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (2.32.5)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (2.10.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (0.38.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.38.0) (11.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.0->gradio==3.38.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.0->gradio==3.38.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.0->gradio==3.38.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.0->gradio==3.38.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.0->gradio==3.38.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.0->gradio==3.38.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.0->gradio==3.38.0) (1.22.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==3.38.0) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==3.38.0) (2.10.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client>=0.2.10->gradio==3.38.0) (2025.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.14.0->gradio==3.38.0) (3.20.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.14.0->gradio==3.38.0) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.14.0->gradio==3.38.0) (1.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.0.0->markdown-it-py[linkify]>=2.0.0->gradio==3.38.0) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.38.0) (2.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==3.38.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==3.38.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==3.38.0) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==3.38.0) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==3.38.0) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==3.38.0) (2.9.0.post0)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio==3.38.0)\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "INFO: pip is still looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio==3.38.0)\n",
            "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio==3.38.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio==3.38.0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.38.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.38.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.38.0) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.0->gradio==3.38.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests~=2.0->gradio==3.38.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.0->gradio==3.38.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests~=2.0->gradio==3.38.0) (2025.10.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.14.0->gradio==3.38.0) (8.3.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.14.0->gradio==3.38.0) (0.16.0)\n",
            "Requirement already satisfied: starlette<0.50.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi->gradio==3.38.0) (0.49.1)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi->gradio==3.38.0) (0.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->gradio==3.38.0) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->gradio==3.38.0) (1.0.9)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.38.0) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.38.0) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.38.0) (0.28.0)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.12/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.38.0) (1.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==3.38.0) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx->gradio==3.38.0) (1.3.1)\n",
            "Downloading gradio-3.38.0-py3-none-any.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow, numpy, markupsafe, markdown-it-py, aiofiles, mdit-py-plugins, gradio\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.3\n",
            "    Uninstalling MarkupSafe-3.0.3:\n",
            "      Successfully uninstalled MarkupSafe-3.0.3\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 4.0.0\n",
            "    Uninstalling markdown-it-py-4.0.0:\n",
            "      Successfully uninstalled markdown-it-py-4.0.0\n",
            "  Attempting uninstall: aiofiles\n",
            "    Found existing installation: aiofiles 24.1.0\n",
            "    Uninstalling aiofiles-24.1.0:\n",
            "      Successfully uninstalled aiofiles-24.1.0\n",
            "  Attempting uninstall: mdit-py-plugins\n",
            "    Found existing installation: mdit-py-plugins 0.5.0\n",
            "    Uninstalling mdit-py-plugins-0.5.0:\n",
            "      Successfully uninstalled mdit-py-plugins-0.5.0\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.49.1\n",
            "    Uninstalling gradio-5.49.1:\n",
            "      Successfully uninstalled gradio-5.49.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unstructured-client 0.42.3 requires aiofiles>=24.1.0, but you have aiofiles 23.2.1 which is incompatible.\n",
            "pi-heif 1.1.1 requires pillow>=11.1.0, but you have pillow 10.4.0 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "yfinance 0.2.66 requires websockets>=13.0, but you have websockets 11.0.3 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 gradio-3.38.0 markdown-it-py-2.2.0 markupsafe-2.1.5 mdit-py-plugins-0.3.3 numpy-1.26.4 pillow-10.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "numpy"
                ]
              },
              "id": "0e3fb5e69bec4f8fa1ca2626db94d615"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain-pinecone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgZfFjTebKhm",
        "outputId": "43353b83-4790-4f57-ed30-fa20f7114cb6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/587.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m583.7/587.6 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/259.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.3/259.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/65.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet pinecone"
      ],
      "metadata": {
        "id": "A3ZYbAC-bcIn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Environment Variables and Initialize core components"
      ],
      "metadata": {
        "id": "u4zR9btVYqW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import PyPDFLoader , TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Pinecone as langChainPineCone\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "OPENAI_API_KEY = getpass.getpass('OpenAI API Key:')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "PINECONE_API_KEY = getpass.getpass('Pinecone API Key:')\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,      # Max characters per chunk\n",
        "    chunk_overlap=200,    # Characters of overlap between chunks\n",
        "    length_function=len   # How to measure chunk size (using len() for characters)\n",
        ")\n",
        "\n",
        "# Initialize the pineconeClient\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# Your index name\n",
        "INDEX_NAME = \"smart-study-buddy-index\"\n",
        "\n",
        "# 2. Get the embedding dimension\n",
        "# OpenAI provides: small = 1536, large = 3072\n",
        "#OpenAI text-embedding-3-small → 1,536-dimensional embeddings\n",
        "#OpenAI text-embedding-3-large → 3,072-dimensional embeddings\n",
        "\n",
        "EMBEDDING_DIMENSION = 1536\n",
        "METRIC = \"cosine\"\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# 3. Create the index (your snippet)\n",
        "try:\n",
        "     # Check if the index already exists\n",
        "    existing_indexes = pc.list_indexes()\n",
        "    print(f\"Existing Indexes: {existing_indexes}\")\n",
        "\n",
        "    if INDEX_NAME not in [index.name for index in existing_indexes]:\n",
        "        print(f\"Index '{INDEX_NAME}' does not exist. Creating new index...\")\n",
        "        pc.create_index(\n",
        "          name=INDEX_NAME,\n",
        "          dimension=EMBEDDING_DIMENSION,\n",
        "          metric=METRIC,\n",
        "          spec=ServerlessSpec(\n",
        "                cloud=\"aws\",        # or \"gcp\"\n",
        "                region=\"us-east-1\"  # pick the same region as your Pinecone project\n",
        "            )\n",
        "       )\n",
        "        print(f\"Successfully created new index: '{INDEX_NAME}' with dimension {EMBEDDING_DIMENSION} and metric '{METRIC}'.\")\n",
        "    else:\n",
        "        print(f\"Using existing index: '{INDEX_NAME}'\")\n",
        "    # Connect to the index (this is more for direct operations, LangChain will also connect)\n",
        "    index = pc.Index(INDEX_NAME)\n",
        "    print(f\"Successfully connected to index '{INDEX_NAME}'.\")\n",
        "    print(f\"Index stats: {index.describe_index_stats()}\")\n",
        "\n",
        "except Exception as e:\n",
        "   print(f\"Error creating/connecting to Pinecone index '{INDEX_NAME}': {str(e)}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOwF_ZqrYpbd",
        "outputId": "cce9e88e-c046-4f40-dbad-7da3d15a54ee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API Key:··········\n",
            "Pinecone API Key:··········\n",
            "Existing Indexes: [{\n",
            "    \"name\": \"my-embedding-index\",\n",
            "    \"metric\": \"cosine\",\n",
            "    \"host\": \"my-embedding-index-czeoi6f.svc.aped-4627-b74a.pinecone.io\",\n",
            "    \"spec\": {\n",
            "        \"serverless\": {\n",
            "            \"cloud\": \"aws\",\n",
            "            \"region\": \"us-east-1\"\n",
            "        }\n",
            "    },\n",
            "    \"status\": {\n",
            "        \"ready\": true,\n",
            "        \"state\": \"Ready\"\n",
            "    },\n",
            "    \"vector_type\": \"dense\",\n",
            "    \"dimension\": 1536,\n",
            "    \"deletion_protection\": \"disabled\",\n",
            "    \"tags\": null\n",
            "}]\n",
            "Index 'smart-study-buddy-index' does not exist. Creating new index...\n",
            "Successfully created new index: 'smart-study-buddy-index' with dimension 1536 and metric 'cosine'.\n",
            "Successfully connected to index 'smart-study-buddy-index'.\n",
            "Index stats: {'dimension': 1536,\n",
            " 'index_fullness': 0.0,\n",
            " 'metric': 'cosine',\n",
            " 'namespaces': {},\n",
            " 'total_vector_count': 0,\n",
            " 'vector_type': 'dense'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Documents and Chunking"
      ],
      "metadata": {
        "id": "vbGNd3b--Re9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Load Documents and Chunk them\n",
        "#4. Data Ingestion & Processing:\n",
        "\n",
        "SMART_STUDY_BUDDY_DIR = \"./smart_study_buddy_dir\"\n",
        "\n",
        "# Create directory if it doesn't exist (and inform the user)\n",
        "if not os.path.exists(SMART_STUDY_BUDDY_DIR):\n",
        "    os.makedirs(SMART_STUDY_BUDDY_DIR)\n",
        "    print(f\"Created input directory: {SMART_STUDY_BUDDY_DIR}. Please add your sample lecture note files (.txt, .pdf) to this folder.\")\n",
        "elif not os.listdir(SMART_STUDY_BUDDY_DIR):\n",
        "    print(f\"Input directory {SMART_STUDY_BUDDY_DIR} is empty. Please add your sample lecture notes to this folder for the Study Buddy to work.\")\n",
        "\n",
        "\n",
        "all_documents = []\n",
        "if(os.path.exists(SMART_STUDY_BUDDY_DIR) and os.listdir(SMART_STUDY_BUDDY_DIR)):\n",
        "  for filename in os.listdir(SMART_STUDY_BUDDY_DIR):\n",
        "    print(f\"loading the document {filename}\")\n",
        "    file_path = os.path.join(SMART_STUDY_BUDDY_DIR, filename)\n",
        "    try:\n",
        "      if(filename.endswith(\".pdf\")):\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        loader_docs = loader.load()\n",
        "        print(f\"Loaded PDF : {filename} , with number of pages : {len(loader_docs)}\")\n",
        "        all_documents.extend(loader_docs)\n",
        "      elif(filename.endswith(\".txt\")):\n",
        "        loader = TextLoader(file_path)\n",
        "        loader_docs = loader.load()\n",
        "        print(f\"Loaded TXT : {filename} , with number of pages : {len(loader_docs)}\")\n",
        "        all_documents.extend(loader_docs)\n",
        "      else:\n",
        "        print(f\"-- Skipped unsupported file: {filename}\")\n",
        "        continue\n",
        "    except Exception as e:\n",
        "            print(f\"Error loading file {filename}: {e}\")\n",
        "\n",
        "if all_documents:\n",
        "  print(f\"Total number of documents loaded : {len(all_documents)}\")\n",
        "else:\n",
        "  print(\"No processable documents were found. \")\n",
        "\n",
        "\n",
        "# Chunking : Split the documents into chunking\n",
        "\n",
        "chunks = []\n",
        "\n",
        "if all_documents:\n",
        "  chunks = text_splitter.split_documents(all_documents)\n",
        "  print(f\"Total number of chunks : {len(chunks)}\")\n",
        "else:\n",
        "  print(\"No processable documents were found. \")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQvaLjw45o4A",
        "outputId": "a8738bb9-12d5-4fc9-d15c-46d01dd81dfc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading the document Software_Design_Patterns_for_AI-Systems.pdf\n",
            "Loaded PDF : Software_Design_Patterns_for_AI-Systems.pdf , with number of pages : 7\n",
            "loading the document nosqldb.pdf\n",
            "Loaded PDF : nosqldb.pdf , with number of pages : 29\n",
            "loading the document history_lecture_1.txt\n",
            "Loaded TXT : history_lecture_1.txt , with number of pages : 1\n",
            "Total number of documents loaded : 37\n",
            "Total number of chunks : 52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Vector Store"
      ],
      "metadata": {
        "id": "01XwEoym-VCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "import time\n",
        "\n",
        "vectorstore = None\n",
        "\n",
        "if chunks:\n",
        "  print(f\"Generating Embeddings {len(chunks)} chunks and storing them at  pinecone Index: {INDEX_NAME}\")\n",
        "  try:\n",
        "    vectorstore = PineconeVectorStore.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        index_name=INDEX_NAME,\n",
        "        text_key=\"text\"\n",
        "    )\n",
        "    print(f\"Successfully stored embeddings in Pinecone index '{INDEX_NAME}'.\")\n",
        "    # It might take a few moments for Pinecone's stats to update after upserting.\n",
        "    time.sleep(10) # Give Pinecone a moment to update stats\n",
        "    stats = pc.Index(INDEX_NAME).describe_index_stats()\n",
        "    print(f\"\\nUpdated Index Statistics for '{INDEX_NAME}':\")\n",
        "    print(f\"Total vectors: {stats.total_vector_count}\")\n",
        "    print(f\"Namespaces: {stats.namespaces}\")\n",
        "\n",
        "  except Exception as e:\n",
        "   print(f\"Error creating/connecting to Pinecone index '{INDEX_NAME}': {str(e)}\")\n",
        "else:\n",
        "    print(\"\\nNo chunks were created from documents. Trying to connect to an existing Pinecone index for Q&A...\")\n",
        "    try:\n",
        "      vectorstore = PineconeVectorStore.from_existing_index(\n",
        "        embedding=embeddings,\n",
        "        index_name=INDEX)\n",
        "    except Exception as e:\n",
        "      print(f\"Error creating/connecting to Pinecone index '{INDEX_NAME}': {str(e)}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJYNqf7a8dEf",
        "outputId": "76d159d5-ef75-4cbe-953f-89313bc0abd9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Embeddings 52 chunks and storing them at  pinecone Index: smart-study-buddy-index\n",
            "Successfully stored embeddings in Pinecone index 'smart-study-buddy-index'.\n",
            "\n",
            "Updated Index Statistics for 'smart-study-buddy-index':\n",
            "Total vectors: 104\n",
            "Namespaces: {'': {'vector_count': 104}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Brain : Question and Answering with Session Memory"
      ],
      "metadata": {
        "id": "TLngMrHgBpvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = None\n",
        "\n",
        "if vectorstore:\n",
        "  # initialize LLM\n",
        "  llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "  print(f\"llm initialized with model name : {llm.model_name}\")\n",
        "\n",
        "  #Create Retriever from pinecode vector store\n",
        "  retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "\n",
        "  print(f\"retriver created for pinecone index : {INDEX_NAME}. And retrieveing top 3 chunks\")\n",
        "\n",
        "  prompt_template_str = \"\"\"\n",
        "    You are a helpful AI Smart Study Buddy. Use the following pieces of context from lecture notes and the chat history to answer the question at the end.\n",
        "    Your goal is to answer the user's question based *only* on the provided lecture notes context.\n",
        "    Do not use any external knowledge or make up information.\n",
        "    If the answer to the question cannot be found in the provided context, clearly state \"I'm sorry, but I couldn't find information about that in your lecture notes.\"\n",
        "    If the context is empty or irrelevant to the question, also state that you cannot find the answer in the notes.\n",
        "\n",
        "    Context from lecture notes:\n",
        "    {context}\n",
        "\n",
        "    Chat History:\n",
        "    {chat_history}\n",
        "\n",
        "    Question: {question}\n",
        "    Helpful Answer from your lecture notes:\n",
        "    \"\"\"\n",
        "  qa_prompt = PromptTemplate(\n",
        "        input_variables=[\"context\", \"chat_history\", \"question\"],\n",
        "        template=prompt_template_str)\n",
        "\n",
        "  print(\"prompt template defined\")\n",
        "\n",
        "  #Initialize Conversation Memory\n",
        "\n",
        "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True,output_key='answer')\n",
        "\n",
        "  #Conversation Memory Initialized\n",
        "  print(\"Conversation Memory Initialized\")\n",
        "\n",
        "  qa_chain = ConversationalRetrievalChain.from_llm(llm=llm,\n",
        "                                                    retriever=retriever,\n",
        "                                                    memory=memory,\n",
        "                                                    combine_docs_chain_kwargs={\"prompt\": qa_prompt},\n",
        "                                                    verbose=True,\n",
        "                                                   return_source_documents=True)\n",
        "\n",
        "  print(\"Conversation QA Chain Initialized\")\n",
        "else:\n",
        "  print(\"Vectore store for pinecone not available. skipping conversation chain setup\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRJdZM2yBulF",
        "outputId": "e8f635a4-c7ba-44e4-9642-1e4cfaff2b9a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llm initialized with model name : gpt-3.5-turbo\n",
            "retriver created for pinecone index : smart-study-buddy-index. And retrieveing top 3 chunks\n",
            "prompt template defined\n",
            "Conversation Memory Initialized\n",
            "Conversation QA Chain Initialized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2728344993.py:38: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True,output_key='answer')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chating with your Smart Studdy Buddy"
      ],
      "metadata": {
        "id": "FndaZTKFHW7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_buddy(query:str):\n",
        "  global chat_history  # uses the chat history managed by memory\n",
        "  if qa_chain:\n",
        "\n",
        "    try:\n",
        "      print(f\" User question : {query}\")\n",
        "      result = qa_chain.invoke({\"question\": query}) # ChatHistory implicitly managed by memory object\n",
        "      answer = result['answer']\n",
        "      print(f\"Smart Study Buddy Answer : {answer}\")\n",
        "\n",
        "      #Soruce Documents\n",
        "      print(\"\\nSource Documents:\")\n",
        "      if result.get('source_documents'):\n",
        "        for i,doc in enumerate(result['source_documents']):\n",
        "          source_name = doc.metadata.get('source', 'Unknown source')\n",
        "           # Truncate page_content for display\n",
        "          content_preview = doc.page_content.replace('\\n', ' ').strip()[:150]\n",
        "          print(f\"  {i+1}. Source: {source_name}\\n     Content Preview: '{content_preview}...'\")\n",
        "      else:\n",
        "          print(\"  No specific source documents were heavily relied upon or returned by the retriever.\")\n",
        "\n",
        "      return answer\n",
        "    except Exception as e:\n",
        "      print(f\"Error in chat_with_buddy: {e}\")\n",
        "  else:\n",
        "     print(\"QA chain is not initialized. Cannot process query. Please check previous steps.\")\n",
        "     return \"Error: QA chain not set up.\"\n"
      ],
      "metadata": {
        "id": "l72HH_fOHagx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Test Scenarios ---\n",
        "\n",
        "if qa_chain:\n",
        "  #starting chat session with smart studdy buddy\n",
        "  question1 = \"What are common themes in Shakespearean tragedies?\"\n",
        "  response1 = chat_with_buddy(question1)\n",
        "  print(\"\\n\")\n",
        "\n",
        "  if response1 and \"Error:\" not in response1  and \"I am sorry\" not in response1:\n",
        "    print(response1)\n",
        "    print(\"\\n\")\n",
        "    question2 = \"Can you name a key character in Hamlet?\"\n",
        "    response2 = chat_with_buddy(question2)\n",
        "\n",
        "    print(\"Question 2: \\n\")\n",
        "    print(response2)\n",
        "    print(\"\\n\")\n",
        "  else:\n",
        "     print(\"\\nSkipping chat example as the QA chain is not set up. Please check previous steps.\")"
      ],
      "metadata": {
        "id": "j2hAGTyNJoSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "demo = gr.ChatInterface(chat_with_buddy,\n",
        "                        chatbot=gr.Chatbot(height=200),\n",
        "                        textbox=gr.Textbox(placeholder=\"Hi I am your Smart Study Buddy, How I can help you today?\", container=False, scale=7),\n",
        "                        title=\"Smart Study Buddy\",\n",
        "                        theme=\"soft\",\n",
        "                        examples=[\"What are common themes in Shakespearean tragedies?\"],\n",
        "                        retry_btn=None,\n",
        "                        undo_btn=\"Delete Previous\")\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "My82QoheMhwQ",
        "outputId": "8e709e45-afbd-4be2-8cc5-6da5097e8b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:95: UserWarning: The function to ChatInterface should take two inputs (message, history) and return a single string response.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMPORTANT: You are using gradio version 3.38.0, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://faefad54b0c12dfc07.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://faefad54b0c12dfc07.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}